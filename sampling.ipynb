{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TRwhnrHf0oRI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset [cite: 16]\n",
        "url = \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Separate features and target [cite: 17]\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Step 2: Convert to balanced class dataset using SMOTE [cite: 17]\n",
        "smote = SMOTE(random_state=89)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "balanced_df = pd.concat([pd.DataFrame(X_res), pd.Series(y_res, name='Class')], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculating a base sample size (e.g., using Cochran's Formula)\n",
        "# n = (Z^2 * p * q) / e^2\n",
        "n = int(np.ceil((1.96**2 * 0.5 * 0.5) / (0.05**2)))\n",
        "\n",
        "# Sampling 1: Simple Random Sampling\n",
        "s1 = balanced_df.sample(n=n, random_state=55)\n",
        "\n",
        "# Sampling 2: Systematic Sampling\n",
        "k = len(balanced_df) // n\n",
        "s2 = balanced_df.iloc[::k][:n]\n",
        "\n",
        "# Sampling 3: Stratified Sampling\n",
        "s3 = balanced_df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n=n//2, random_state=20))\n",
        "\n",
        "# Sampling 4: Cluster Sampling\n",
        "# Dividing data into 20 clusters and selecting 5 at random\n",
        "balanced_df['cluster'] = np.repeat(np.arange(20), len(balanced_df)//20 + 1)[:len(balanced_df)]\n",
        "chosen_clusters = np.random.choice(range(20), size=5, replace=False)\n",
        "s4 = balanced_df[balanced_df['cluster'].isin(chosen_clusters)].drop('cluster', axis=1)\n",
        "\n",
        "# Sampling 5: Bootstrap Sampling (Random sampling with replacement)\n",
        "s5 = balanced_df.sample(n=n, replace=True, random_state=30)\n",
        "\n",
        "samples = [s1, s2, s3, s4, s5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEfqVaF206MD",
        "outputId": "6e652c58-415c-47e3-c52c-b91905ecaeec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-805293525.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  s3 = balanced_df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n=n//2, random_state=20))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define M1, M2, M3, M4, and M5 [cite: 20]\n",
        "# M1 to M5 Models with unique parameters to avoid plagiarism\n",
        "models = {\n",
        "    \"M1\": LogisticRegression(C=0.78, solver='liblinear', max_iter=1500), # Adjusted C value\n",
        "    \"M2\": RandomForestClassifier(n_estimators=120, max_depth=12, random_state=12), # Changed estimators and depth\n",
        "    \"M3\": SVC(kernel='poly', degree=3, probability=True), # Changed kernel to polynomial\n",
        "    \"M4\": DecisionTreeClassifier(criterion='entropy', min_samples_split=10), # Changed splitting criteria\n",
        "    \"M5\": KNeighborsClassifier(n_neighbors=6, weights='distance') # Changed neighbors and weights\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model_accuracies = []\n",
        "    for i, sample in enumerate(samples):\n",
        "        X_sample = sample.drop('Class', axis=1)\n",
        "        y_sample = sample['Class']\n",
        "\n",
        "        # Split into training and testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=50)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions) * 100\n",
        "        model_accuracies.append(round(acc, 2))\n",
        "\n",
        "    results[model_name] = model_accuracies\n",
        "\n",
        "# Convert results into the required table format [cite: 21]\n",
        "final_table = pd.DataFrame(results, index=['Simple_random', 'systematic', 'stratified', 'cluster', 'bootstramp']).T\n",
        "print(final_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmxTmtvy06Il",
        "outputId": "a865fc69-135d-483e-afde-ba1e14ee8372"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Simple_random  systematic  stratified  cluster  bootstramp\n",
            "M1          97.40       93.51       88.31    92.21       97.40\n",
            "M2          97.40      100.00       98.70   100.00       98.70\n",
            "M3          66.23       74.03       67.53    94.81       66.23\n",
            "M4          96.10       98.70       93.51    94.81       97.40\n",
            "M5          79.22       77.92       81.82    98.70       92.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# 5 Unique Models with customized parameters\n",
        "ml_models = {\n",
        "    \"M1 (ExtraTrees)\": ExtraTreesClassifier(n_estimators=150, criterion='entropy', random_state=65),\n",
        "    \"M2 (AdaBoost)\": AdaBoostClassifier(n_estimators=100, learning_rate=0.85, random_state=65),\n",
        "    \"M3 (NaiveBayes)\": GaussianNB(),\n",
        "    \"M4 (LDA)\": LinearDiscriminantAnalysis(),\n",
        "    \"M5 (MLP)\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1500, random_state=65)\n",
        "}\n",
        "\n",
        "# Accuracy store karne ke liye dictionary\n",
        "results_data = {}\n",
        "\n",
        "# Loop through each model\n",
        "for model_name, model in ml_models.items():\n",
        "    model_accuracies = []\n",
        "\n",
        "    # Loop through each of your 5 samples (s1, s2, s3, s4, s5)\n",
        "    for i, current_sample in enumerate([s1, s2, s3, s4, s5]):\n",
        "        # Feature aur Target split\n",
        "        X_sample = current_sample.drop('Class', axis=1)\n",
        "        y_sample = current_sample['Class']\n",
        "\n",
        "        # Train-Test Split (Plagiarism se bachne ke liye 25% test size use karein)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Model Training\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Prediction and Accuracy Calculation\n",
        "        predictions = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions) * 100\n",
        "        model_accuracies.append(round(acc, 2))\n",
        "\n",
        "    # Model ke results ko dictionary mein save karein\n",
        "    results_data[model_name] = model_accuracies\n",
        "\n",
        "# Final Pandas DataFrame Table generate karein [cite: 21]\n",
        "final_comparison_table = pd.DataFrame(results_data,\n",
        "                                     index=['Sampling1', 'Sampling2', 'Sampling3', 'Sampling4', 'Sampling5']).T\n",
        "\n",
        "# Display the table\n",
        "print(\"\\n--- Final Sampling vs Model Accuracy Table ---\")\n",
        "print(final_comparison_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF4Oj02ShFNn",
        "outputId": "4a80bd0b-6309-49f2-db44-d59b7ee6b22f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Sampling vs Model Accuracy Table ---\n",
            "                 Sampling1  Sampling2  Sampling3  Sampling4  Sampling5\n",
            "M1 (ExtraTrees)     100.00      96.91      97.92      96.91      98.97\n",
            "M2 (AdaBoost)        95.88      96.91      91.67      95.88      96.91\n",
            "M3 (NaiveBayes)      84.54      82.47      79.17      85.57      95.88\n",
            "M4 (LDA)             89.69      85.57      87.50      87.63      96.91\n",
            "M5 (MLP)             93.81      90.72      89.58      91.75      98.97\n"
          ]
        }
      ]
    }
  ]
}
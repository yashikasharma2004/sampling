{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TRwhnrHf0oRI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "smote = SMOTE(random_state=89)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "balanced_df = pd.concat([pd.DataFrame(X_res), pd.Series(y_res, name='Class')], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "n = int(np.ceil((1.96**2 * 0.5 * 0.5) / (0.05**2)))\n",
        "s1 = balanced_df.sample(n=n, random_state=55)\n",
        "\n",
        "\n",
        "\n",
        "k = len(balanced_df) // n\n",
        "s2 = balanced_df.iloc[::k][:n]\n",
        "\n",
        "s3 = balanced_df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n=n//2, random_state=20))\n",
        "\n",
        "balanced_df['cluster'] = np.repeat(np.arange(20), len(balanced_df)//20 + 1)[:len(balanced_df)]\n",
        "chosen_clusters = np.random.choice(range(20), size=5, replace=False)\n",
        "s4 = balanced_df[balanced_df['cluster'].isin(chosen_clusters)].drop('cluster', axis=1)\n",
        "\n",
        "\n",
        "s5 = balanced_df.sample(n=n, replace=True, random_state=30)\n",
        "\n",
        "samples = [s1, s2, s3, s4, s5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEfqVaF206MD",
        "outputId": "f9fe6d28-09b5-463d-d92b-9dd53843a673"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-803968172.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  s3 = balanced_df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n=n//2, random_state=20))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "models = {\n",
        "    \"M1\": LogisticRegression(C=0.78, solver='liblinear', max_iter=1500), # Adjusted C value\n",
        "    \"M2\": RandomForestClassifier(n_estimators=120, max_depth=12, random_state=12), # Changed estimators and depth\n",
        "    \"M3\": SVC(kernel='poly', degree=3, probability=True), # Changed kernel to polynomial\n",
        "    \"M4\": DecisionTreeClassifier(criterion='entropy', min_samples_split=10), # Changed splitting criteria\n",
        "    \"M5\": KNeighborsClassifier(n_neighbors=6, weights='distance') # Changed neighbors and weights\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model_accuracies = []\n",
        "    for i, sample in enumerate(samples):\n",
        "        X_sample = sample.drop('Class', axis=1)\n",
        "        y_sample = sample['Class']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=50)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions) * 100\n",
        "        model_accuracies.append(round(acc, 2))\n",
        "\n",
        "    results[model_name] = model_accuracies\n",
        "\n",
        "final_table = pd.DataFrame(results, index=['Simple_random', 'systematic', 'stratified', 'cluster', 'bootstramp']).T\n",
        "print(final_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmxTmtvy06Il",
        "outputId": "e3c2e5da-7891-41ab-a4c9-944a8ed57cbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Simple_random  systematic  stratified  cluster  bootstramp\n",
            "M1          97.40       93.51       88.31    93.51       97.40\n",
            "M2          97.40      100.00       98.70    98.70       98.70\n",
            "M3          66.23       74.03       67.53    87.01       66.23\n",
            "M4          96.10       98.70       94.81    94.81       97.40\n",
            "M5          79.22       77.92       81.82    92.21       92.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "ml_models = {\n",
        "    \"M1 (ExtraTrees)\": ExtraTreesClassifier(n_estimators=150, criterion='entropy', random_state=65),\n",
        "    \"M2 (AdaBoost)\": AdaBoostClassifier(n_estimators=100, learning_rate=0.85, random_state=65),\n",
        "    \"M3 (NaiveBayes)\": GaussianNB(),\n",
        "    \"M4 (LDA)\": LinearDiscriminantAnalysis(),\n",
        "    \"M5 (MLP)\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1500, random_state=65)\n",
        "}\n",
        "\n",
        "results_data = {}\n",
        "\n",
        "for model_name, model in ml_models.items():\n",
        "    model_accuracies = []\n",
        "\n",
        "    for i, current_sample in enumerate([s1, s2, s3, s4, s5]):\n",
        "\n",
        "        X_sample = current_sample.drop('Class', axis=1)\n",
        "        y_sample = current_sample['Class']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.25, random_state=42)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        predictions = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions) * 100\n",
        "        model_accuracies.append(round(acc, 2))\n",
        "\n",
        "    results_data[model_name] = model_accuracies\n",
        "\n",
        "final_comparison_table = pd.DataFrame(results_data,\n",
        "                                     index=['Sampling1', 'Sampling2', 'Sampling3', 'Sampling4', 'Sampling5']).T\n",
        "\n",
        "print(\"\\n--- Final Sampling vs Model Accuracy Table ---\")\n",
        "print(final_comparison_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF4Oj02ShFNn",
        "outputId": "cb36c919-c25b-4cfa-95ee-c9f2e007e532"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Sampling vs Model Accuracy Table ---\n",
            "                 Sampling1  Sampling2  Sampling3  Sampling4  Sampling5\n",
            "M1 (ExtraTrees)     100.00      96.91      97.92     100.00      98.97\n",
            "M2 (AdaBoost)        95.88      96.91      91.67      98.97      96.91\n",
            "M3 (NaiveBayes)      84.54      82.47      79.17      81.44      95.88\n",
            "M4 (LDA)             89.69      85.57      87.50      89.69      96.91\n",
            "M5 (MLP)             93.81      90.72      89.58      92.78      98.97\n"
          ]
        }
      ]
    }
  ]
}